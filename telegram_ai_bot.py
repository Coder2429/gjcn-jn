import os
import logging
from aiogram import Bot, Dispatcher, executor, types
from aiogram.contrib.fsm_storage.memory import MemoryStorage
from aiogram.dispatcher import FSMContext
from aiogram.dispatcher.filters.state import State, StatesGroup
import openai
import replicate
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
REPLICATE_API_TOKEN = os.getenv("REPLICATE_API_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID")  # –ù–∞–ø—Ä–∏–º–µ—Ä, "@your_channel"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
bot = Bot(token=TELEGRAM_TOKEN)
storage = MemoryStorage()
dp = Dispatcher(bot, storage=storage)

# –°–æ—Å—Ç–æ—è–Ω–∏—è FSM (Finite State Machine)
class PostGeneration(StatesGroup):
    waiting_for_text_prompt = State()
    waiting_for_image_prompt = State()

# –ö–æ–º–∞–Ω–¥–∞ /start
@dp.message_handler(commands=['start'])
async def cmd_start(message: types.Message):
    await message.reply(
        "ü§ñ –ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å—Ç–æ–≤ —Å –ò–ò.\n"
        "–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n"
        "/generate_post - –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –ø–æ—Å—Ç\n"
        "/help - –ü–æ–º–æ—â—å"
    )

# –ö–æ–º–∞–Ω–¥–∞ /generate_post
@dp.message_handler(commands=['generate_post'])
async def cmd_generate_post(message: types.Message):
    await PostGeneration.waiting_for_text_prompt.set()
    await message.reply("üìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ù–∞–ø–∏—à–∏ –ø–æ—Å—Ç –ø—Ä–æ –ò–ò'):")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI
@dp.message_handler(state=PostGeneration.waiting_for_text_prompt)
async def process_text_prompt(message: types.Message, state: FSMContext):
    async with state.proxy() as data:
        data['text_prompt'] = message.text

    await PostGeneration.next()
    await message.reply("üé® –¢–µ–ø–µ—Ä—å –≤–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–§—É—Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –≥–æ—Ä–æ–¥'):")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏—è
@dp.message_handler(state=PostGeneration.waiting_for_image_prompt)
async def process_image_prompt(message: types.Message, state: FSMContext):
    async with state.proxy() as data:
        data['image_prompt'] = message.text

    await message.reply("üîÑ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –ø–æ—Å—Ç...")

    try:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        text = generate_text(data['text_prompt'])
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_url = generate_image(data['image_prompt'])
        
        # –ü—É–±–ª–∏–∫–∞—Ü–∏—è –≤ –∫–∞–Ω–∞–ª
        await bot.send_photo(
            chat_id=CHANNEL_ID,
            photo=image_url,
            caption=text
        )
        await message.reply("‚úÖ –ü–æ—Å—Ç —É—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω –≤ –∫–∞–Ω–∞–ª!")
    except Exception as e:
        await message.reply(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        await state.finish()

# –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ (OpenAI)
def generate_text(prompt):
    openai.api_key = OPENAI_API_KEY
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500
    )
    return response.choices[0].message.content

# –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (Replicate/Stable Diffusion)
def generate_image(prompt):
    output = replicate.run(
        "stability-ai/stable-diffusion:ac732df83cea7fff18b8472768c88ad041fa750ff7682a21affe81863cbe77e4",
        input={"prompt": prompt, "width": 512, "height": 512}
    )
    return output[0]

# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
if __name__ == '__main__':
    executor.start_polling(dp, skip_updates=True)